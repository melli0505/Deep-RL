{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "Adam._name = 'AdamOptimizer'\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(5, start=-2)\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        self.state = 38 + random.randint(-5,5)\n",
    "        self.shower_length = 60\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Training forward function\n",
    "        - 실제 simulation 환경에서 fan 속도 제어 명령을 보낼 곳\n",
    "        - reward를 받아올 때까지 시간 delay가 필요함\n",
    "\n",
    "        Args:\n",
    "            action (int, sample): 팬 속도 제어\n",
    "\n",
    "        Returns:\n",
    "            state: _description_\n",
    "            reward: action에 따른 보상\n",
    "            done: episode 종료 여부 표시\n",
    "            info: .\n",
    "        \"\"\"\n",
    "        self.state += action -1 \n",
    "        self.shower_length -= 1 \n",
    "        \n",
    "        # Calculating the reward\n",
    "        if self.state >=37 and self.state <=39: \n",
    "            reward =1 \n",
    "        else: \n",
    "            reward = -1 \n",
    "        \n",
    "        # Checking if shower is done\n",
    "        if self.shower_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Setting the placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Returning the step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 38 + random.randint(-5,5)\n",
    "        self.shower_length = 60 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation=\"linear\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dk866\\anaconda3\\envs\\dqn\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = ENV()\n",
    "\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1,), 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 24)                48        \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 5)                 125       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 773 (3.02 KB)\n",
      "Trainable params: 773 (3.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=60000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Concatenate, Input\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "Adam._name = 'AdamOptimizer'\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.policy import Policy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(5, start=-2)\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        self.state = 38 + random.randint(-5,5)\n",
    "        self.shower_length = 60\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Training forward function\n",
    "        - 실제 simulation 환경에서 fan 속도 제어 명령을 보낼 곳\n",
    "        - reward를 받아올 때까지 시간 delay가 필요함\n",
    "\n",
    "        Args:\n",
    "            action (int, sample): 팬 속도 제어\n",
    "\n",
    "        Returns:\n",
    "            state: _description_\n",
    "            reward: action에 따른 보상\n",
    "            done: episode 종료 여부 표시\n",
    "            info: .\n",
    "        \"\"\"\n",
    "        self.state += action -1 \n",
    "        self.shower_length -= 1 \n",
    "        \n",
    "        # Calculating the reward\n",
    "        if self.state >=37 and self.state <=39: \n",
    "            reward =1 \n",
    "        else: \n",
    "            reward = -1 \n",
    "        \n",
    "        # Checking if shower is done\n",
    "        if self.shower_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Setting the placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Returning the step information\n",
    "        return [self.state], reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_actor(actions):\n",
    "    actor = Sequential()    \n",
    "    actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    actor.add(Dense(16))\n",
    "    actor.add(Activation('relu'))\n",
    "    actor.add(Dense(16))\n",
    "    actor.add(Activation('relu'))\n",
    "    actor.add(Dense(16))\n",
    "    actor.add(Activation('relu'))\n",
    "    actor.add(Dense(actions))\n",
    "    actor.add(Activation('linear'))\n",
    "    \n",
    "    print(actor.summary())\n",
    "    return actor\n",
    "\n",
    "def build_critic(actions):\n",
    "    action_input = Input(shape=(actions,), name='action_input')\n",
    "    print(\"action_input:\", action_input.shape)\n",
    "    observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "    print(\"observation_input: \", observation_input.shape)\n",
    "    flattened_observation = Flatten()(observation_input)\n",
    "    print(\"flattened_observation: \", flattened_observation.shape)\n",
    "    x = Concatenate()([action_input, flattened_observation])\n",
    "    x = Dense(32)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('linear')(x)\n",
    "    critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "\n",
    "    print(critic.summary())\n",
    "    return action_input, critic\n",
    "\n",
    "def build_agent(actions):\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    actor = build_actor(actions)\n",
    "    action_input, critic = build_critic(actions)\n",
    "    \n",
    "    random_process = OrnsteinUhlenbeckProcess(size=actions, theta=.15, mu=0., sigma=.3)\n",
    "    ddpg = DDPGAgent(nb_actions=actions, actor=actor, critic=critic, critic_action_input=action_input, memory=memory, \n",
    "                     nb_steps_warmup_actor=100, nb_steps_warmup_critic=100, random_process=random_process, gamma=.99, target_model_update=1e-3)\n",
    "    return ddpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 1)                 0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                32        \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 85        \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 661 (2.58 KB)\n",
      "Trainable params: 661 (2.58 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "action_input: (None, 5)\n",
      "observation_input:  (None, 1, 1)\n",
      "flattened_observation:  (None, 1)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " observation_input (InputLa  [(None, 1, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " action_input (InputLayer)   [(None, 5)]                  0         []                            \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)         (None, 1)                    0         ['observation_input[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 6)                    0         ['action_input[0][0]',        \n",
      " )                                                                   'flatten_4[0][0]']           \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 32)                   224       ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 32)                   0         ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 32)                   1056      ['activation_15[0][0]']       \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, 32)                   0         ['dense_17[0][0]']            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 32)                   1056      ['activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " activation_17 (Activation)  (None, 32)                   0         ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 1)                    33        ['activation_17[0][0]']       \n",
      "                                                                                                  \n",
      " activation_18 (Activation)  (None, 1)                    0         ['dense_19[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2369 (9.25 KB)\n",
      "Trainable params: 2369 (9.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dk866\\anaconda3\\envs\\dqn\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "c:\\Users\\dk866\\anaconda3\\envs\\dqn\\Lib\\site-packages\\keras\\src\\optimizers\\legacy\\adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Training for 60000 steps ...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m ddpg \u001b[39m=\u001b[39m build_agent(actions)\n\u001b[0;32m      7\u001b[0m ddpg\u001b[39m.\u001b[39mcompile(Adam(lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, clipnorm\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m ddpg\u001b[39m.\u001b[39;49mfit(env, nb_steps\u001b[39m=\u001b[39;49m\u001b[39m60000\u001b[39;49m, visualize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\dk866\\anaconda3\\envs\\dqn\\Lib\\site-packages\\rl\\core.py:134\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mprocess_observation(observation)\n\u001b[1;32m--> 134\u001b[0m \u001b[39massert\u001b[39;00m observation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# Perform random starts at beginning of episode and do not record them into the experience.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m# This slightly changes the start position between games.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m nb_random_start_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m nb_max_start_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(nb_max_start_steps)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = ENV()\n",
    "\n",
    "actions = env.action_space.n\n",
    "print(actions)\n",
    "\n",
    "ddpg = build_agent(actions)\n",
    "ddpg.compile(Adam(lr=1e-3, clipnorm=1.), metrics=['mae'])\n",
    "ddpg.fit(env, nb_steps=60000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PendulumEnv' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(ENV_NAME)\n\u001b[0;32m     18\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m123\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m env\u001b[39m.\u001b[39;49mseed(\u001b[39m123\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     21\u001b[0m nb_actions \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dk866\\anaconda3\\envs\\dqn\\Lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "File \u001b[1;32mc:\\Users\\dk866\\anaconda3\\envs\\dqn\\Lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "File \u001b[1;32mc:\\Users\\dk866\\anaconda3\\envs\\dqn\\Lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PendulumEnv' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "\n",
    "ENV_NAME = 'Pendulum-v1'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "assert len(env.action_space.shape) == 1\n",
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(16))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('linear'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = Concatenate()([action_input, flattened_observation])\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "agent.fit(env, nb_steps=50000, visualize=True, verbose=1, nb_max_episode_steps=200)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('ddpg_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
